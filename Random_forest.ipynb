{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fa4618f",
   "metadata": {},
   "source": [
    "# Decision trees and Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e575cee4",
   "metadata": {},
   "source": [
    "In this notebook, we explain what a random forest is and when to use it. We then write a simple code using some data set on sklearn to see how it performs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f71fb9",
   "metadata": {},
   "source": [
    "First, let’s introduce the problem.\n",
    "\n",
    "We are given a dataset represented by a matrix $X \\in \\mathbb{R}^{N \\times d}$, where:\n",
    "- $N$ is the number of samples (or data points),\n",
    "- $d$ is the number of features (or input variables).\n",
    "\n",
    "Each sample has an associated target value, $y_i$, which are collected in a target vector $y$.\n",
    "\n",
    "We then **split the dataset** into two subsets:\n",
    "- a **training set**, used to fit a model (e.g., linear regression or random forest), and\n",
    "- a **testing set**, used to evaluate the model’s performance.\n",
    "\n",
    "The model is trained on the training set and then assessed on the test set by comparing its predictions to the actual values in $y$. This helps us estimate how well the model is likely to perform on unseen data.\n",
    "\n",
    "\n",
    "We have two types of problems\n",
    "\n",
    "We typically deal with two types of problems:\n",
    "\n",
    "- **Classification**: In this case, the target vector $y$ takes discrete values from a finite set of size $K$, often represented as $\\{0, 1, \\dots, K-1\\}$.\n",
    "  \n",
    "- **Regression**: Here, $y$ takes continuous values in $\\mathbb{R}$.\n",
    "\n",
    "## Decision Tree\n",
    "\n",
    "We first discuss the **decision tree**, and then its generalization: the **random forest**.\n",
    "\n",
    "Let:\n",
    "- $X_{\\text{train}}, X_{\\text{test}}$ be the two partitions of the input data $X$, where the first is used for training and the second for testing;\n",
    "- $y_{\\text{train}}, y_{\\text{test}}$ be the corresponding partitions of the labels $y$.\n",
    "\n",
    "---\n",
    "\n",
    "The goal is to find the best way to split $X_{\\text{train}}$ into smaller subsets that are increasingly **pure** — meaning they contain samples of only one class. We do this recursively.\n",
    "\n",
    "We consider **midpoint splitting**. For a fixed feature index $j$, we sort all the values $X^{(i, j)}_{\\text{train}}$ across samples $i$. This gives a sorted list of values for that feature.\n",
    "\n",
    "We then compute a splitting point $t_{ij}$ at each midpoint:\n",
    "\n",
    "$$\n",
    "t_{ij} = \\frac{1}{2}\\left(X^{(i,j)}_{\\text{train}} + X^{(i+1,j)}_{\\text{train}}\\right)\n",
    "$$\n",
    "\n",
    "For each candidate threshold $t_{ij}$, we compute the **Gini impurity** (for classification):\n",
    "\n",
    "$$\n",
    "G(i,j) = \\frac{n_L(i,j)}{n(i,j)} G_L(i,j) + \\frac{n_R(i,j)}{n(i,j)} G_R(i,j)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $G_L$, $G_R$ are the Gini impurities of the left and right partitions;\n",
    "- $n(i,j)$ is the total number of samples;\n",
    "- $n_L(i,j)$, $n_R(i,j)$ are the number of samples going to the left and right of threshold $t_{ij}$, respectively.\n",
    "\n",
    "The Gini impurity for a set is given by:\n",
    "\n",
    "$$\n",
    "G = 1 - \\sum_{k=1}^K p_k^2\n",
    "$$\n",
    "\n",
    "where $p_k$ is the proportion of samples in class $k$, and $K$ is the total number of classes.\n",
    "\n",
    "We search over all $(t_{ij},j)$ pairs and choose the pair $(t^*, j^*)$ that minimizes $G(i,j)$. This determines the **first split**.\n",
    "\n",
    "We now have a bipartition of the data based on whether a sample's feature $j^*$ is greater or less than $t^*$. These subsets are called the **left child** and **right child**.\n",
    "\n",
    "The splitting process is repeated **recursively** on each child node, stopping when:\n",
    "- The node is **pure** (all samples have the same label, so $G = 0$);\n",
    "- The **maximum tree depth** is exceeded;\n",
    "- The node contains **too few samples** (below a predefined threshold).\n",
    "\n",
    "\n",
    "For **regression**, we use the **mean squared error (MSE)** as the splitting criterion rather than the **Gini impurity** used in classification problems.\n",
    "\n",
    "The **MSE** is defined as:\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{n} \\sum_{i=1}^{n}(y_i - \\hat{y})^2\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $y_i$ is the true target value for the $i$-th sample.\n",
    "- $\\hat{y}$ is the predicted target value for the node (usually the **mean** target value of the samples in that node).\n",
    "- $n$ is the number of samples in the node.\n",
    "\n",
    "After computing the MSE for each possible split, we choose the split $t^*$ that minimizes the MSE or maximizes the variance reduction. We then proceed with the splitting procedure, recursively applying the same process at each node.\n",
    "\n",
    "When a node cannot be split any further (due to stopping criteria such as maximum tree depth, minimum number of samples per node, or if the node is pure), we assign $\\hat{y}$ to that node, which is the mean of the target values of the samples in the node.\n",
    "\n",
    "Thus, the final prediction for a new sample is obtained by traversing the tree, starting at the root node and following the splits until reaching a leaf node. The predicted value for the sample is then the value $\\hat{y}$ assigned to the leaf node.\n",
    "\n",
    "### Key Points:\n",
    "1. **Splitting Criterion**: For regression trees, we use **MSE** or **SSE** to choose the best splits.\n",
    "2. **Prediction**: The predicted value for a new sample is the mean target value $\\hat{y}$ of the samples in the leaf node where the sample falls.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "For example, a decision tree for classifying flowers into 3 species might look like:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28263eb9",
   "metadata": {},
   "source": [
    "       (petal length ≤ 2.45?)\n",
    "              /         \\\n",
    "          Yes             No\n",
    "     (leaf: class 0)    (petal width ≤ 1.75?)\n",
    "                          /       \\\n",
    "                      Yes         No\n",
    "                (leaf: class 1) (leaf: class 2)\n",
    "\n",
    "\n",
    "\n",
    "Once the tree is trained, we classify a new sample $x$ by following the decision rules down the tree until we reach a **leaf node**, where a class label is assigned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4eeb4f",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "Random Forest is an **ensemble learning method** that builds on decision trees by combining the predictions of **multiple trees** to improve accuracy and reduce overfitting.\n",
    "\n",
    "### Main Concept\n",
    "\n",
    "Instead of using a single decision tree:\n",
    "\n",
    "- For **classification**: predictions are made by **majority vote** across all trees.\n",
    "- For **regression**: predictions are made by taking the **average** across all trees.\n",
    "\n",
    "This reduces the tendency of a single tree to overfit the data.\n",
    "\n",
    "---\n",
    "\n",
    "###  How the Forest is trained\n",
    "\n",
    "#### 1. **Bootstrapping (Bagging)**\n",
    "\n",
    "- For each tree, we generate a **random sample _with replacement_** from the original dataset.\n",
    "- This process is called **bootstrapping**.\n",
    "- We repeat this for $n_{trees}$, training one tree on each sample. We have a collection of $n_{trees}$ number of trees: a forest.\n",
    "\n",
    "#### 2. **Feature Bagging**\n",
    "\n",
    "- At each **split** in a tree, we choose the best split from a **random subset of features** rather than all features.\n",
    "- This further introduces randomness and reduces correlation between trees.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a792feed",
   "metadata": {},
   "source": [
    "# Codes\n",
    "We now use sklearn to write two codes:\n",
    "* Classification code: we check the accuracy of random forest vs a single decision tree\n",
    "* Regression code: we check the accuracy of random forest vs a linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a286c429",
   "metadata": {},
   "source": [
    "# Classification code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9edab341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Decision Tree: 0.94\n",
      "Accuracy of Random Forest: 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "data = load_wine()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 1. Train a single decision tree\n",
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "y_pred_tree = tree_clf.predict(X_test)\n",
    "tree_accuracy = accuracy_score(y_test, y_pred_tree)\n",
    "\n",
    "# 2. Train a random forest\n",
    "forest_clf = RandomForestClassifier(random_state=42)\n",
    "forest_clf.fit(X_train, y_train)\n",
    "y_pred_forest = forest_clf.predict(X_test)\n",
    "forest_accuracy = accuracy_score(y_test, y_pred_forest)\n",
    "\n",
    "# Accuracy\n",
    "print(f\"Accuracy of Decision Tree: {tree_accuracy:.2f}\")\n",
    "print(f\"Accuracy of Random Forest: {forest_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87d1bdb",
   "metadata": {},
   "source": [
    "We can see that the random forest has a better accuracy than a single decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1457bb9",
   "metadata": {},
   "source": [
    "# Regression code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14fe8c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression MSE: 0.5559\n",
      "Random Forest MSE: 0.2554\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "data = fetch_california_housing()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Linear Regression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "\n",
    "# Random Forest Regressor\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# Compare Mean Squared Errors\n",
    "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "\n",
    "print(f\"Linear Regression MSE: {mse_lr:.4f}\")\n",
    "print(f\"Random Forest MSE: {mse_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc4abc3",
   "metadata": {},
   "source": [
    "## Linear Regression vs Random Forest\n",
    "\n",
    "The random forest typically has a lower **mean squared error (MSE)** than linear regression. However, there are some drawbacks.\n",
    "\n",
    "On one hand, in linear regression we have a clear model:\n",
    "\n",
    "$$ \\hat{y} = X\\beta \\implies \\hat{y}_i = \\sum_{j=1}^{d} X_{ij} \\beta_j + b $$\n",
    "\n",
    "Here, $X$ is the $N \\times (d+1)$ **design matrix**, where $N$ is the number of samples and $d$ is the number of features. Each row of $X$ encodes the features of a sample, and there is an additional column of ones that corresponds to the constant intercept $b$ of the model.\n",
    "\n",
    "This implies that we have a clear interpretation of our results: the best-fitting parameter $\\beta$ minimizes the MSE, assuming the model is linear.\n",
    "\n",
    "On the other hand, the random forest **incorporates non-linear features**, but we do not have an explicit expression for the model $\\hat{y}$ in terms of $X$. This makes it a **non-parametric** method and makes interpretation harder.\n",
    "\n",
    "While random forests often perform better in terms of predictive accuracy, especially in capturing complex patterns, they lack the interpretability that linear regression provides.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
